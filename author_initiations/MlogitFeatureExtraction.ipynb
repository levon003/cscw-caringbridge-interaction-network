{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"With whom do users initiate?\" Mlogit Modeling\n",
    "===\n",
    "\n",
    "Multiple notes in other places about this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import bisect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/srivbane/levon003/repos/qual-health-journeys/annotation_data\")\n",
    "import journal as journal_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/author_initiations\"\n",
    "assert os.path.exists(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/panfs/roc/groups/3/srivbane/levon003/repos/sna-social-support/figures'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = git_root_dir[0]\n",
    "figures_dir = os.path.join(git_root_dir, 'figures')\n",
    "figures_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.fromisoformat('2005-01-01')\n",
    "start_timestamp = int(start_date.timestamp() * 1000)\n",
    "end_date = datetime.fromisoformat('2016-06-01')\n",
    "end_timestamp = int(end_date.timestamp() * 1000)\n",
    "subset_start_date = datetime.fromisoformat('2014-01-01')\n",
    "subset_start_timestamp = int(subset_start_date.timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362345"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the list of valid users\n",
    "data_selection_working_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/data_selection\"\n",
    "valid_user_ids = set()\n",
    "with open(os.path.join(data_selection_working_dir, \"valid_user_ids.txt\"), 'r') as infile:\n",
    "    for line in infile:\n",
    "        user_id = line.strip()\n",
    "        if user_id == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            valid_user_ids.add(int(user_id))\n",
    "len(valid_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the list of valid sites\n",
    "data_selection_working_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/data_selection\"\n",
    "valid_site_ids = set()\n",
    "with open(os.path.join(data_selection_working_dir, \"valid_site_ids.txt\"), 'r') as infile:\n",
    "    for line in infile:\n",
    "        site_id = line.strip()\n",
    "        if site_id == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            valid_site_ids.add(int(site_id))\n",
    "len(valid_site_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:10.179630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15850052"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the journal metadata with author type info added\n",
    "s = datetime.now()\n",
    "author_type_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "journal_metadata_filepath = os.path.join(author_type_dir, \"journal_metadata_with_author_type.df\")\n",
    "journal_df = pd.read_feather(journal_metadata_filepath)\n",
    "print(datetime.now() - s)\n",
    "len(journal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a quick fix for invalid dates in journals, when created_at is 0 we use the updated_at instead\n",
    "# note that only 41 updates have this issue\n",
    "invalid_created_at = journal_df.created_at <= 0\n",
    "journal_df.loc[invalid_created_at, 'created_at'] = journal_df.loc[invalid_created_at, 'updated_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_cond_filepath = os.path.join(\"/home/lana/shared/caringbridge/data/projects/sna-social-support/user_metadata\", \"assigned_health_conditions.feather\")\n",
    "user_health_conds_df = pd.read_feather(health_cond_filepath)\n",
    "len(user_health_conds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362345"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the user author type dataframe\n",
    "author_type_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/author_type\"\n",
    "user_patient_proportions_filepath = os.path.join(author_type_dir, 'user_patient_proportions.df')\n",
    "user_df = pd.read_feather(user_patient_proportions_filepath)\n",
    "len(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11424980"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the user->user interactions dataframe\n",
    "metadata_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/user_metadata\"\n",
    "u2u_df = pd.read_feather(os.path.join(metadata_dir,\"u2u_df.feather\"))\n",
    "len(u2u_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840943"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the site-level metadata\n",
    "site_metadata_working_dir = \"/home/lana/shared/caringbridge/data/derived/site_metadata\"\n",
    "site_metadata_filepath = os.path.join(site_metadata_working_dir, \"site_metadata.feather\")\n",
    "site_metadata_df = pd.read_feather(site_metadata_filepath)\n",
    "len(site_metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently not necessary, since we use the processed user->user interactions...\n",
    "# read in the interactions dataframe\n",
    "#metadata_dir = \"/home/lana/shared/caringbridge/data/projects/sna-social-support/user_metadata\"\n",
    "#author_to_site = os.path.join(metadata_dir, \"interaction_metadata.h5\")\n",
    "#ints_df = pd.read_hdf(author_to_site)\n",
    "#len(ints_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and merge the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362345"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = user_df[user_df.user_id.isin(valid_user_ids)]\n",
    "len(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05158343567594419"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['is_multisite_author'] = user_df.num_sites > 1\n",
    "np.sum(user_df.is_multisite_author) / len(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79115, 0.2183416357339)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_mixedsite_author_dict = {}\n",
    "site_author_sets = journal_df[journal_df.user_id.isin(valid_user_ids)].groupby('site_id').agg({'user_id': lambda user_ids: set(user_ids)})\n",
    "for site_id, user_ids in zip(site_author_sets.index, site_author_sets.user_id):\n",
    "    if len(user_ids) > 1:\n",
    "        for user_id in user_ids:\n",
    "            is_mixedsite_author_dict[user_id] = True\n",
    "is_mixedsite_author = [user_id in is_mixedsite_author_dict for user_id in user_df.user_id]\n",
    "user_df['is_mixedsite_author'] = is_mixedsite_author\n",
    "# 21.8% of authors have written updates on a site on which another valid author has written an update \n",
    "np.sum(is_mixedsite_author), np.sum(is_mixedsite_author) / len(is_mixedsite_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in the health condition data\n",
    "user_health_cond_dict = {user_id: assigned_health_cond for user_id, assigned_health_cond in zip(user_health_conds_df.user_id, user_health_conds_df.assigned_health_cond)}\n",
    "health_condition = [user_health_cond_dict[user_id] for user_id in user_df.user_id]\n",
    "user_df['health_condition'] = health_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of journal updates, first update, last update\n",
    "user_updates_df = journal_df[journal_df.user_id.isin(valid_user_ids)].groupby('user_id').agg({\n",
    "    'journal_oid': lambda group: len(group),\n",
    "    'created_at': lambda created_ats: (np.min(created_ats), np.max(created_ats))\n",
    "}).reset_index()  # note that columns are not renamed appropriately, but are reused immediately\n",
    "user_update_count_dict = {\n",
    "    user_id: count for user_id, count \n",
    "    in zip(user_updates_df.user_id, user_updates_df.journal_oid)}\n",
    "user_first_update_dict = {\n",
    "    user_id: created_at[0] for user_id, created_at \n",
    "    in zip(user_updates_df.user_id, user_updates_df.created_at)}\n",
    "user_last_update_dict = {\n",
    "    user_id: created_at[1] for user_id, created_at \n",
    "    in zip(user_updates_df.user_id, user_updates_df.created_at)}\n",
    "update_count = [user_update_count_dict[user_id] for user_id in user_df.user_id]\n",
    "first_update = [user_first_update_dict[user_id] for user_id in user_df.user_id]\n",
    "last_update = [user_last_update_dict[user_id] for user_id in user_df.user_id]\n",
    "user_df['update_count'] = update_count\n",
    "user_df['first_update'] = first_update\n",
    "user_df['last_update'] = last_update\n",
    "user_df['author_tenure'] = user_df.last_update - user_df.first_update\n",
    "assert np.all(user_df.author_tenure > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posting frequency (updates per month, across all sites)\n",
    "tenure_in_months = user_df.author_tenure / (1000 * 60 * 60 * 24 * 30)\n",
    "user_df['update_frequency'] = user_df.update_count / tenure_in_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_interacted_with\n",
    "# computed from the user->user interaction data\n",
    "interacted_with_user_ids = set(u2u_df.to_user_id)\n",
    "is_interacted_with = [user_id in interacted_with_user_ids for user_id in user_df.user_id]\n",
    "user_df['is_interacted_with'] = is_interacted_with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207746, 0.573337565027805)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(is_interacted_with), np.sum(is_interacted_with) / len(is_interacted_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is this user an initiator at any point\n",
    "initiating_user_ids = set(u2u_df.from_user_id)\n",
    "is_initiator = [user_id in initiating_user_ids for user_id in user_df.user_id]\n",
    "user_df['is_initiator'] = is_initiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206978, 0.5712180380576523)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(is_initiator), np.sum(is_initiator) / len(is_initiator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the dictionary for user->(created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_updates_dict = journal_df.sort_values(by='created_at', ascending=True).groupby('user_id').agg({\n",
    "    'created_at': lambda created_at: created_at.tolist()\n",
    "}).created_at.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the visits of the most-visited site authored by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15850052/15850052 [00:43<00:00, 364755.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# construct user->site dictionary\n",
    "# contains all sites that authors have updated at least one journal update on\n",
    "user_site_dict = defaultdict(set)\n",
    "for row in tqdm(journal_df.itertuples(), total=len(journal_df)):\n",
    "    user_site_dict[row.user_id].add(row.site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct site->visits dictionary\n",
    "site_visits_dict = {site_id: visits for site_id, visits in zip(site_metadata_df.site_id, site_metadata_df.visits)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct user->visits dictionary\n",
    "# pools across multiple sites by taking the site with the maximum number of visits\n",
    "user_visits_dict = {user_id: max(site_visits_dict[site_id] for site_id in user_site_dict[user_id] if site_id in site_visits_dict) \n",
    " for user_id in user_df.user_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the u2u links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10102765, 0.8842698192907121)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_u2u_df = u2u_df[(u2u_df.from_user_id.isin(valid_user_ids))&(u2u_df.to_user_id.isin(valid_user_ids))]\n",
    "len(valid_u2u_df), len(valid_u2u_df) / len(u2u_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(932616, 0.08162955208674326)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inits_df = valid_u2u_df.sort_values(by='created_at', ascending=True).drop_duplicates(subset=['from_user_id', 'to_user_id'], keep='first')\n",
    "len(inits_df), len(inits_df) / len(u2u_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start_date = datetime.fromisoformat('2014-01-01')\n",
    "model_start_timestamp = int(model_start_date.timestamp() * 1000)\n",
    "model_end_date = datetime.fromisoformat('2016-01-01')\n",
    "model_end_timestamp = int(model_end_date.timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of high-level graph code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WccGraph:\n",
    "    def __init__(self, node_uids):\n",
    "        self.node_uids = node_uids\n",
    "        self.node_dict = {}  # maps node_uid to component_uid\n",
    "        self.component_dict = {}  # maps component_uid to a set of node_uids\n",
    "        for component_uid, node_uid in enumerate(self.node_uids):\n",
    "            self.node_dict[node_uid] = component_uid\n",
    "            self.component_dict[component_uid] = set((node_uid,))\n",
    "        self.edge_count = 0\n",
    "        \n",
    "    def add_edge(self, from_node_uid, to_node_uid):\n",
    "        self.edge_count += 1\n",
    "        from_component_uid = self.node_dict[from_node_uid]\n",
    "        to_component_uid = self.node_dict[to_node_uid]\n",
    "        if from_component_uid == to_component_uid:\n",
    "            # these nodes are already weakly connected\n",
    "            is_intra_component_edge = True\n",
    "            from_component_size, to_component_size = 0, 0\n",
    "        else:  # two different components are being merged with this edge\n",
    "            is_intra_component_edge = False\n",
    "            from_component_nodes = self.component_dict[from_component_uid]\n",
    "            to_component_nodes = self.component_dict[to_component_uid]\n",
    "            from_component_size = len(from_component_nodes)\n",
    "            to_component_size = len(to_component_nodes)\n",
    "            \n",
    "            if from_component_size >= to_component_size:\n",
    "                # merge To component into From component, deleting the To component\n",
    "                from_component_nodes.update(to_component_nodes)\n",
    "                del self.component_dict[to_component_uid]\n",
    "                for node_uid in to_component_nodes:\n",
    "                    # update the merged in component ids\n",
    "                    self.node_dict[node_uid] = from_component_uid\n",
    "            else:\n",
    "                # merge From component into To component, deleting the From component\n",
    "                to_component_nodes.update(from_component_nodes)\n",
    "                del self.component_dict[from_component_uid]\n",
    "                for node_uid in from_component_nodes:\n",
    "                    # update the merged in component ids\n",
    "                    self.node_dict[node_uid] = to_component_uid\n",
    "        return is_intra_component_edge, from_component_size, to_component_size\n",
    "    \n",
    "    def are_weakly_connected(self, user_id1, user_id2):\n",
    "        # two nodes are weakly connected if they exist in the same WCC\n",
    "        return self.node_dict[user_id1] == self.node_dict[user_id2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_connected(G, source, target):\n",
    "    nodes = []\n",
    "    visited = set()\n",
    "    visited.add(source)\n",
    "    nodes.extend(G[source])\n",
    "    are_connected = False\n",
    "    while len(nodes) != 0:\n",
    "        node = nodes.pop(0)\n",
    "        if node in visited:\n",
    "            continue\n",
    "        visited.add(node)\n",
    "        if node == target:\n",
    "            are_connected = True\n",
    "            break\n",
    "        nodes.extend(G[node])\n",
    "    return are_connected\n",
    "\n",
    "def are_strongly_connected(G, user_id1, user_id2):\n",
    "    # we assume that user_id1 and user_id2 are known to be weakly connected\n",
    "    # thus, the two are strongly connected if we can find a path from one to the other and back\n",
    "    if len(G[user_id1]) == 0 or len(G[user_id2]) == 0:\n",
    "        # if there are zero outbound edges from one of the nodes, they can't be strongly connected\n",
    "        return False\n",
    "    return are_connected(G, user_id1, user_id2) and are_connected(G, user_id2, user_id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_is_friend_of_friend(G, user_id1, user_id2):\n",
    "    if len(G[user_id1]) == 0 or len(G[user_id2]) == 0:\n",
    "        # if there are zero outbound edges from one of the nodes, they can't be strongly connected\n",
    "        return False\n",
    "    return are_fof_connected(G, user_id1, user_id2) and are_fof_connected(G, user_id2, user_id1)\n",
    "\n",
    "def are_fof_connected(G, source, target):\n",
    "    # must be a direct connection from either source -> target, or from source -> neighbor -> target\n",
    "    if target in G[source]:\n",
    "        return True\n",
    "    for neighbor in G[source]:\n",
    "        if target in G[neighbor]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the initial graph subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720917"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inits_subset = inits_df[inits_df.created_at < model_start_timestamp]\n",
    "len(inits_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.800987\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now()\n",
    "base_graph = nx.DiGraph()\n",
    "nodes = set(inits_subset.from_user_id) | set(inits_subset.to_user_id)\n",
    "edges = [tuple(row) for row in inits_subset[[\"from_user_id\", \"to_user_id\"]].values]\n",
    "base_graph.add_nodes_from(nodes)\n",
    "base_graph.add_edges_from(edges)\n",
    "print(f\"{datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.281152\n"
     ]
    }
   ],
   "source": [
    "# this second graph tracks only weakly connected component info\n",
    "s = datetime.now()\n",
    "user_set = set(inits_df.from_user_id) | set(inits_df.to_user_id)\n",
    "wcc_graph = WccGraph(user_set)\n",
    "for from_user_id, to_user_id in inits_subset[[\"from_user_id\", \"to_user_id\"]].values:\n",
    "    wcc_graph.add_edge(from_user_id, to_user_id)\n",
    "print(f\"{datetime.now() - s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = base_graph.copy()  # okay to edit this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_user_id</th>\n",
       "      <th>to_user_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>int_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6880404</th>\n",
       "      <td>6665650</td>\n",
       "      <td>3761912</td>\n",
       "      <td>1388556206000</td>\n",
       "      <td>amps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416195</th>\n",
       "      <td>11120609</td>\n",
       "      <td>26768219</td>\n",
       "      <td>1388567467000</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4123254</th>\n",
       "      <td>2780134</td>\n",
       "      <td>12973638</td>\n",
       "      <td>1388588006000</td>\n",
       "      <td>amps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7344625</th>\n",
       "      <td>7754587</td>\n",
       "      <td>16500310</td>\n",
       "      <td>1388590554000</td>\n",
       "      <td>amps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400034</th>\n",
       "      <td>152211</td>\n",
       "      <td>1709758</td>\n",
       "      <td>1388594784000</td>\n",
       "      <td>guestbook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         from_user_id  to_user_id     created_at   int_type\n",
       "6880404       6665650     3761912  1388556206000       amps\n",
       "8416195      11120609    26768219  1388567467000    comment\n",
       "4123254       2780134    12973638  1388588006000       amps\n",
       "7344625       7754587    16500310  1388590554000       amps\n",
       "400034         152211     1709758  1388594784000  guestbook"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20000\n",
    "s = 24\n",
    "# sample n initiations\n",
    "# using s negative samples\n",
    "# valid candidate users are ALL valid authors who have posted their first update at this time\n",
    "inits_subset = inits_df[(inits_df.created_at >= model_start_timestamp)&(inits_df.created_at <= model_end_timestamp)]\n",
    "inits_subset = inits_subset.sample(n=n).sort_values(by='created_at', ascending=True)\n",
    "inits_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7502187142088341"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df['time_to_first_update'] = user_df.first_update - model_start_timestamp\n",
    "# if first update is positive, it is still in the future\n",
    "# if first update is <= 0, then it should already be an eligible node\n",
    "# however, it might not be in the network, since the base network only contains connected nodes\n",
    "active_user_ids = user_df.loc[user_df.time_to_first_update <= 0, 'user_id']\n",
    "len(active_user_ids) / len(user_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153742"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create data structures storing all of the edges that do not yet but will exist in the model\n",
    "# these will be added incrementally as computation continues\n",
    "model_subset = inits_df[(inits_df.created_at >= model_start_timestamp)&(inits_df.created_at <= model_end_timestamp)]\n",
    "all_edges = [(created_at, tuple(row))\n",
    "             for created_at, row \n",
    "             in zip(model_subset.created_at, model_subset[[\"from_user_id\", \"to_user_id\"]].values)]\n",
    "edge_df = pd.DataFrame(all_edges, columns=['created_at', 'edge'])\n",
    "edge_df['time_to_existence'] = edge_df.created_at - model_start_timestamp\n",
    "# if time_to_existence <= 0, it should exist in the network\n",
    "assert np.all(edge_df.time_to_existence > 0)\n",
    "len(edge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'is_strongly_connected_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-428dcb0c9263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mis_reciprocal_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_reciprocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mis_weakly_connected_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_weakly_connected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mis_strongly_connected_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_strongly_connected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mis_friend_of_friend_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_friend_of_friend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_strongly_connected_list' is not defined"
     ]
    }
   ],
   "source": [
    "prev_timestep = model_start_timestamp\n",
    "active_user_ids = user_df.loc[user_df.time_to_first_update <= 0, 'user_id']\n",
    "sampled_initiations = []\n",
    "for from_user_id, to_user_id, created_at in tqdm(zip(inits_subset.from_user_id, inits_subset.to_user_id, inits_subset.created_at), total=len(inits_subset)):\n",
    "    curr_timestep = created_at\n",
    "    elapsed_time = curr_timestep - prev_timestep\n",
    "    if elapsed_time > 0:  # if 2+ sampled initiations occur at the same time, elapsed_time == 0\n",
    "        # update the active users set\n",
    "        user_df.time_to_first_update -= elapsed_time\n",
    "        active_user_ids = user_df.loc[user_df.time_to_first_update <= 0, 'user_id']\n",
    "        # update the graph with all initiations between previous timestep and now\n",
    "        edge_df.time_to_existence -= elapsed_time\n",
    "        new_edge_mask = edge_df.time_to_existence < 0  # edges that exist AT zero happen at the current timestep, including the edge from_user_id, to_user_id\n",
    "        new_edges = edge_df[new_edge_mask]\n",
    "        edge_df = edge_df[~new_edge_mask] # TODO Use loc for assignment?\n",
    "        #assert np.all(edge_df[edge_df.time_to_existence==0].created_at == created_at)\n",
    "        G.add_edges_from(new_edges.edge)\n",
    "        # also add edges to the WCC graph\n",
    "        for from_user_id, to_user_id in new_edges.edge:\n",
    "            wcc_graph.add_edge(from_user_id, to_user_id)\n",
    "    \n",
    "    # candidate users are all active users...\n",
    "    candidate_user_ids = set(active_user_ids)\n",
    "    # ... minus the true initiation target...\n",
    "    candidate_user_ids.discard(to_user_id)\n",
    "    # ... minus users already initiated to by this user\n",
    "    if from_user_id in G:\n",
    "        candidate_user_ids -= set(G[from_user_id].keys())\n",
    "    \n",
    "    # we only sample s of the candidate users\n",
    "    negative_sampled_users = list(random.sample(candidate_user_ids, s))\n",
    "    \n",
    "    # now, extract ids for the target user and all of the negative sampled users\n",
    "    indegree_list = []\n",
    "    outdegree_list = []\n",
    "    is_reciprocal_list = []\n",
    "    is_weakly_connected_list = []\n",
    "    is_friend_of_friend_list = []\n",
    "    #is_strongly_connected_list = []\n",
    "    for user_id in [to_user_id] + negative_sampled_users:\n",
    "        is_friend_of_friend = False\n",
    "        if user_id in G:\n",
    "            indegree = G.in_degree(user_id)\n",
    "            outdegree = G.out_degree(user_id)\n",
    "            is_reciprocal = from_user_id in G[user_id]\n",
    "            is_weakly_connected = wcc_graph.are_weakly_connected(from_user_id, user_id)\n",
    "            if is_weakly_connected:\n",
    "                is_friend_of_friend = compute_is_friend_of_friend(G, from_user_id, user_id)\n",
    "                #is_strongly_connected = are_strongly_connected(G, from_user_id, user_id)\n",
    "        else:\n",
    "            indegree = 0\n",
    "            outdegree = 0\n",
    "            is_reciprocal = False\n",
    "            is_weakly_connected = False\n",
    "        \n",
    "        indegree_list.append(indegree)\n",
    "        outdegree_list.append(outdegree)\n",
    "        is_reciprocal_list.append(is_reciprocal)\n",
    "        is_weakly_connected_list.append(is_weakly_connected)\n",
    "        is_strongly_connected_list.append(is_strongly_connected)\n",
    "        is_friend_of_friend_list.append(is_friend_of_friend)\n",
    "    \n",
    "    d = {\n",
    "        'initiator_user_id': from_user_id,\n",
    "        'target_user_id': to_user_id,\n",
    "        'negative_user_ids': negative_sampled_users,\n",
    "        'created_at': created_at,\n",
    "        'indegree_list': indegree_list,\n",
    "        'outdegree_list': outdegree_list,\n",
    "        'is_reciprocal_list': is_reciprocal_list,\n",
    "        'is_weakly_connected_list': is_weakly_connected_list,\n",
    "        'is_friend_of_friend_list': is_friend_of_friend_list\n",
    "    }\n",
    "    sampled_initiations.append(d)\n",
    "    \n",
    "    prev_timestep = curr_timestep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_inits_df = pd.DataFrame(sampled_initiations)\n",
    "len(sampled_inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sampled initiations dataframe with graph features\n",
    "# so that the expensive graph feature computation can be saved\n",
    "sampled_inits_df_filename = \"sampled_inits_df.pickle\"\n",
    "sampled_inits_df_filepath = os.path.join(working_dir, sampled_inits_df_filename)\n",
    "sampled_inits_df.to_pickle(sampled_inits_df_filepath)\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sampled initiations dataframe with graph features\n",
    "sampled_inits_df_filename = \"sampled_inits_df.pickle\"\n",
    "sampled_inits_df_filepath = os.path.join(working_dir, sampled_inits_df_filename)\n",
    "sampled_inits_df = pd.read_pickle(sampled_inits_df_filepath)\n",
    "len(sampled_inits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_inits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for computing user-level features\n",
    "author_type_dict = {row.user_id: row.user_author_type for row in user_df.itertuples()}\n",
    "health_condition_dict = {row.user_id: row.health_condition for row in user_df.itertuples()}\n",
    "is_multisite_author_dict = {row.user_id: row.is_multisite_author for row in user_df.itertuples()}\n",
    "is_mixedsite_author_dict = {row.user_id: row.is_mixedsite_author for row in user_df.itertuples()}\n",
    "update_count_dict = {row.user_id: row.update_count for row in user_df.itertuples()}\n",
    "update_frequency_dict = {row.user_id: row.update_frequency for row in user_df.itertuples()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute days_since_most_recent_update\n",
    "# given a target user_id and a created_at timestamp\n",
    "def get_most_recent_update(user_id, created_at):\n",
    "    update_times = user_updates_dict[user_id]\n",
    "    # update_times is a sorted list of created_at times for all updates by the given user_id\n",
    "    ind = bisect.bisect_right(update_times, created_at)\n",
    "    most_recent_update = update_times[ind-1]\n",
    "    return most_recent_update\n",
    "\n",
    "def compute_days_since_most_recent_update(user_id, created_at):\n",
    "    most_recent_update = get_most_recent_update(user_id, created_at)\n",
    "    ms_since_most_recent_update = created_at - most_recent_update\n",
    "    days_since_most_recent_update = ms_since_most_recent_update / (1000 * 60 * 60 * 24)\n",
    "    return days_since_most_recent_update\n",
    "\n",
    "def compute_days_since_first_update(user_id, created_at):\n",
    "    update_times = user_updates_dict[user_id]\n",
    "    ind = bisect.bisect_right(update_times, created_at)\n",
    "    most_recent_update = update_times[ind-1]\n",
    "    first_update = update_times[0]\n",
    "    ms_since_first_update = most_recent_update - first_update\n",
    "    days_since_first_update = ms_since_first_update / (1000 * 60 * 60 * 24)\n",
    "    return days_since_first_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_initiations_filename = \"author_initiation_choices_train_20000.csv\"\n",
    "sampled_initiations_filepath = os.path.join(working_dir, sampled_initiations_filename)\n",
    "with open(sampled_initiations_filepath, 'w') as outfile:\n",
    "    header = \"\"\"\n",
    "            choice_id,\n",
    "            initiator_user_id,\n",
    "            candidate_user_id,\n",
    "            is_target,\n",
    "            target_outdegree,\n",
    "            target_indegree,\n",
    "            target_has_indegree,\n",
    "            is_reciprocal,\n",
    "            is_weakly_connected,\n",
    "            is_friend_of_friend,\n",
    "            is_author_type_shared,\n",
    "            target_author_type,\n",
    "            initiator_author_type,\n",
    "            target_health_condition,\n",
    "            is_health_condition_shared,\n",
    "            target_is_multisite_author,\n",
    "            target_is_mixedsite_author,\n",
    "            target_update_count,\n",
    "            target_update_frequency,\n",
    "            target_days_since_most_recent_update,\n",
    "            target_days_since_first_update,\n",
    "            target_site_visits\n",
    "    \"\"\"\n",
    "    header = re.sub(r'\\s+', '', header).strip() + \"\\n\"\n",
    "    format_str = \"iiiiiiiiiiiccciiiidddi\"\n",
    "    outfile.write(header)\n",
    "    for i, row in tqdm(enumerate(sampled_inits_df.itertuples()), total=len(sampled_inits_df)):\n",
    "        choice_id = i\n",
    "        initiator_user_id = row.initiator_user_id\n",
    "        initiator_author_type = author_type_dict[initiator_user_id]\n",
    "        initiator_health_condition = health_condition_dict[initiator_user_id]\n",
    "        for i, user_id in enumerate([row.target_user_id] + row.negative_user_ids):\n",
    "            is_target = int(i == 0)\n",
    "            candidate_user_id = user_id\n",
    "            target_outdegree = row.outdegree_list[i]\n",
    "            target_indegree = row.indegree_list[i]\n",
    "            target_has_indegree = int(target_indegree > 0)\n",
    "            is_reciprocal = int(row.is_reciprocal_list[i])\n",
    "            is_weakly_connected = int(row.is_weakly_connected_list[i])\n",
    "            is_friend_of_friend = int(row.is_friend_of_friend_list[i])\n",
    "            \n",
    "            # Include the user-level features for the candidates\n",
    "            target_author_type = author_type_dict[candidate_user_id]\n",
    "            is_author_type_shared = int(initiator_author_type == target_author_type)\n",
    "            \n",
    "            target_health_condition = health_condition_dict[candidate_user_id]\n",
    "            is_health_condition_shared = int(initiator_health_condition == target_health_condition)\n",
    "            \n",
    "            target_is_multisite_author = int(is_multisite_author_dict[candidate_user_id])\n",
    "            target_is_mixedsite_author = int(is_mixedsite_author_dict[candidate_user_id])\n",
    "            target_update_count = update_count_dict[candidate_user_id]\n",
    "            target_update_frequency = update_frequency_dict[candidate_user_id]\n",
    "            \n",
    "            target_days_since_most_recent_update = compute_days_since_most_recent_update(candidate_user_id, row.created_at)\n",
    "            target_days_since_first_update = compute_days_since_first_update(candidate_user_id, row.created_at)\n",
    "            \n",
    "            target_site_visits = user_visits_dict[candidate_user_id]\n",
    "            \n",
    "            line_vars = [\n",
    "                choice_id,\n",
    "                initiator_user_id,\n",
    "                candidate_user_id,\n",
    "                is_target,\n",
    "                target_outdegree,\n",
    "                target_indegree,\n",
    "                target_has_indegree,\n",
    "                is_reciprocal,\n",
    "                is_weakly_connected,\n",
    "                is_friend_of_friend,\n",
    "                is_author_type_shared,\n",
    "                target_author_type,\n",
    "                initiator_author_type,\n",
    "                target_health_condition,\n",
    "                is_health_condition_shared,\n",
    "                target_is_multisite_author,\n",
    "                target_is_mixedsite_author,\n",
    "                target_update_count,\n",
    "                target_update_frequency,\n",
    "                target_days_since_most_recent_update,\n",
    "                target_days_since_first_update,\n",
    "                target_site_visits\n",
    "            ]\n",
    "            line = \",\".join([str(v) for v in line_vars]) + \"\\n\"\n",
    "            #line = f\"{choice_id},{initiator_user_id},{candidate_user_id},{is_target},{target_outdegree},{target_indegree},{target_has_indegree},{is_reciprocal},{is_author_type_shared},{target_author_type},{initiator_author_type}\\n\"\n",
    "            outfile.write(line)\n",
    "print(f\"R column types format string: {format_str}\")\n",
    "sampled_initiations_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create test set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
