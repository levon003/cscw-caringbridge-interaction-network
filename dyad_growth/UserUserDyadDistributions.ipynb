{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-To-User Dyad Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes analysis for **reciprocal relationships**, **average lengths**, and **power users**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "m_to_d = (1000 * 60 * 60 * 24)\n",
    "\n",
    "metadata_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/user_metadata\"\n",
    "csv_dir = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/csv_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_to_site = os.path.join(metadata_dir, \"interaction_metadata.h5\")\n",
    "df = pd.read_hdf(author_to_site)\n",
    "sorted_df = df.sort_values(by=[\"user_id\", \"site_id\"])\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrivial_df = sorted_df[sorted_df.is_nontrivial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{}% of entries are nontrivial\".format(100 * len(nontrivial_df)/len(sorted_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema for U2U Generation\n",
    "\n",
    "```\n",
    "journals    intx        result\n",
    "u1 -> s1    u3->s1      u3->u1\n",
    "u2 -> s2    u4->s1      u4->u1\n",
    "            u5->s2      u5->u2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = sorted_df[sorted_df.int_type == \"journal\"]\n",
    "len(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = nontrivial_df[(nontrivial_df.user_id > 0) & (nontrivial_df.int_type != \"journal\") & (~nontrivial_df.is_self_interaction)]\n",
    "len(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposion Example:\n",
    "```\n",
    "for int in ints:\n",
    "    (which authors were on this site by this time?)  O(1)\n",
    "        site_id = int.site_id\n",
    "        created_at = int.created_at\n",
    "        authors_dict = site_authors_dict[site_id]\n",
    "        for created_at_key in authors_dict.keys():\n",
    "            if created_at_key <= created_int:\n",
    "                emit a tuple\n",
    " \n",
    "site_authors_dict = \n",
    "    site_id -> {\n",
    "        created_at -> user_id  # the time when this user_id first published an update on this site\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = journals.groupby(by=['site_id','user_id']).agg({'created_at': np.min})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = df.to_dict(orient='records')\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_authors_dict = defaultdict(dict)\n",
    "for i, ind in enumerate(df.index):\n",
    "    site_id, user_id = ind\n",
    "    created_at = x[i]['created_at']\n",
    "    authors_dict = site_authors_dict[site_id]\n",
    "    authors_dict[created_at] = user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Approach 1: Multi-dimensional Lookup (300 hours => 25 minutes)\n",
    "u2u = []\n",
    "for i, row in tqdm(ints.iterrows(), total=len(ints)):\n",
    "    site_id = row.site_id\n",
    "    created_at = row.created_at\n",
    "    authors_dict = site_authors_dict[site_id]\n",
    "    for created_at_key in authors_dict.keys():\n",
    "        if created_at_key <= created_at:\n",
    "            tup = (row.user_id, authors_dict[created_at_key], created_at)\n",
    "            u2u.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Pandas Application function (300 hours => 20 minutes)\n",
    "#def get_tuples(ind):\n",
    "#    site_id = row.site_id\n",
    "#    created_at = row.created_at\n",
    "#    authors_dict = site_authors_dict[site_id]\n",
    "#    for created_at_key in authors_dict.keys():\n",
    "#        if created_at_key <= created_at:\n",
    "#            tup = (row.user_id, authors_dict[created_at_key], created_at)\n",
    "#            u2u.append(tup)\n",
    "# ints.apply(get_tuples, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recips = pd.DataFrame(columns=[\"from\", \"to\", \"length, num_rel\"])\n",
    "# Approach 3: create dicts for each index (300 hours => 5 minutes)\n",
    "#created_at_dict = {ind: created_at for ind, created_at in zip(ints.index, ints.created_at)}\n",
    "#user_id_dict = {ind: user_id for ind, user_id in zip(ints.index, ints.user_id)}\n",
    "#site_id_dict = {ind: user_id for ind, site_id in zip(ints.index, ints.site_id)}\n",
    "\n",
    "#for ind in ints.index:\n",
    "#    site_id = site_id_dict[ind]\n",
    "#    created_at = created_at_dict[ind]\n",
    "#    user_id = user_id_dict[ind]\n",
    "#    authors_dict = site_authors_dict[site_id]\n",
    "#    for created_at_key in authors_dict.keys():\n",
    "#        if created_at_key <= created_at:\n",
    "#            tup = (user_id, authors_dict[created_at_key], created_at)\n",
    "#            u2u.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(u2u, columns = [\"from\", \"to\", \"at\"])\n",
    "df = df.sort_values(by=[\"from\", \"to\", \"at\"])\n",
    "df.reset_index().to_feather(\"u2u_sample.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u_df = pd.read_feather(\"u2u_sample.feather\")[[\"from\", \"to\", \"at\"]]\n",
    "u2u_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Reduce (from, to, at) to (from, to, length, num_relationships) ###########################\n",
    "def reduce_by_user(df):  \n",
    "    i = 0\n",
    "    this_uid = -1\n",
    "    this_to = -1\n",
    "    this_list = []\n",
    "    reduced = []\n",
    "    end_date = -1\n",
    "    start_date = -1\n",
    "    num_rel = 0\n",
    "    for i, row in tqdm(df.iterrows(), total = df.shape[0]):  # for row in u2u\n",
    "        if row[\"from\"] != this_uid:                          # if this is a new user\n",
    "            if this_uid != -1:                               # and not our first\n",
    "                for a in this_list:                          # write out data\n",
    "                    reduced.append((this_uid, a[0], a[1], num_rel))\n",
    "                this_list = []\n",
    "            this_uid = row[\"from\"]                           # reset tracking data\n",
    "            num_rel = 0\n",
    "            end_date = -1\n",
    "            start_date = -1\n",
    "            new = True\n",
    "        if not new:\n",
    "            num_rel += 1\n",
    "            continue\n",
    "        new = False                                          # pick out all of the intx by this user once\n",
    "        reducee = df[df[\"from\"] == this_uid].sort_values(by=[\"from\", \"to\", \"at\"])\n",
    "        for j, to_entry in reducee.iterrows():               # for each intx\n",
    "            if to_entry[\"to\"] != this_to:                    # if the recieving user is new\n",
    "                if this_to != -1:                            # and not the first one\n",
    "                    this_list.append((this_to, end_date - start_date))\n",
    "                this_to = to_entry[\"to\"]                     \n",
    "                start_date = to_entry[\"at\"]                 \n",
    "            end_date = to_entry[\"at\"]                        # stretch out the length for each intx in the pair\n",
    "    for a in this_list: \n",
    "        reduced.append((this_uid, a[0], a[1], num_rel))      # once finished with everything, put on last data\n",
    "    this_list = []\n",
    "    return pd.DataFrame(reduced, columns = [\"from\", \"to\", \"length_d\", \"num_rel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u2u_reduced = reduce_by_user(u2u_df)\n",
    "u2u_reduced.to_feather(\"u2u_reduced.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u = pd.read_hdf(\"revised_u2u.h5\")\n",
    "u2u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recips = pd.DataFrame(columns=[\"from\", \"to\", \"length, num_rel\"])\n",
    "u2u_copy = u2u_reduced.copy()\n",
    "for i, row in u2u_copy.iterrows():\n",
    "    match = u2u_copy[(u2u_copy[\"from\"] == row[\"to\"]) & (u2u_copy[\"to\"] == row[\"from\"])]\n",
    "    if len(match) > 0:\n",
    "        recips.append(match)\n",
    "        u2u_copy.drop(i)\n",
    "        u2u_copy.drop(match.index)\n",
    "nonrecips = u2u_reduced[~u2u_reduced.index.isin(recips.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recips = pd.DataFrame(columns=[\"from\", \"to\", \"created_at\", \"int_type\"])\n",
    "from_ind = {ind: from_uid for ind, from_uid in zip(u2u.index, u2u[\"from_user_id\"])}\n",
    "to_ind = {ind: from_uid for ind, from_uid in zip(u2u.index, u2u[\"to_user_id\"])}\n",
    "at_ind = {ind: from_uid for ind, from_uid in zip(u2u.index, u2u[\"created_at\"])}\n",
    "int_ind = {ind: from_uid for ind, from_uid in zip(u2u.index, u2u[\"int_type\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in tqdm(u2u.index):\n",
    "    from_uid = from_ind[ind]\n",
    "    to_uid = to_ind[ind]\n",
    "    created_at = at_ind[ind]\n",
    "    int_type= int_ind[ind]\n",
    "    match = u2u[(u2u[\"from_user_id\"] == to_uid) & (u2u[\"to_user_id\"] == from_uid)]\n",
    "    if len(match) > 0:\n",
    "        recips.append(match)\n",
    "        u2u.drop(ind)\n",
    "        u2u.drop(match.index)\n",
    "nonrecips = u2u[~u2u.index.isin(recips.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recips.reset_index().to_feather(\"recips.feather\")\n",
    "nonrecips.reset_index().to_feahter(\"nonrecips.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u = pd.read_feather(\"u2u_sample.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u_df = u2u_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = u2u_df.sort_values(by=[\"from\"]) \n",
    "current_uid = -1\n",
    "avgs = []\n",
    "for i, row in avg_df.iterrows():\n",
    "    if current_uid != row[\"from\"]:\n",
    "        avgs.append(avg_df[avg_df[\"from\"] == row[\"from\"]].length_d.mean()) # for each unique user, find their mean\n",
    "    current_uid = row[\"from\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4), dpi= 200, facecolor='w', edgecolor='k')\n",
    "hist, bins, _ = ax[0].hist(np.divide(avgs, ( m_to_d), bins=50, color='black')\n",
    "logbins = np.logspace(np.log10(bins[1]),np.log10(bins[-1]),len(bins))\n",
    "ax[1].hist(np.divide(avgs,  m_to_d), bins=logbins, color='black')\n",
    "ax[0].set_ylabel(\"Quantity\")\n",
    "ax[0].set_xlabel(\"Days of relationship\")\n",
    "ax[0].set_title(\"Dyadic Distribution\")\n",
    "ax[0].set_xlim(0, 4000)\n",
    "ax[1].set_ylabel(\"Quantity\")\n",
    "ax[1].set_xlabel(\"Days of relationship\")\n",
    "ax[1].set_title(\"Logarithmic Dyadic Distribution\")\n",
    "ax[1].set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Users (Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_df = u2u_df.sort_values(by=[\"num_rel\", \"from\"], ascending=False)\n",
    "j = 0;\n",
    "cur_user = -1;\n",
    "power_lengths = [[] for x in range(0,10)]\n",
    "for i, row in power_df.iterrows():\n",
    "    if cur_user != -1 and cur_user != row[\"from\"]:\n",
    "        j += 1\n",
    "    if j < 10:\n",
    "        cur_user = row[\"from\"]\n",
    "        power_lengths[j].append(row[\"length_d\"]) # find top 10 user length hists\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize=(12, 10), dpi= 200, facecolor='w', edgecolor='k')\n",
    "k = 0\n",
    "for i in range(0,2):\n",
    "    for j in range(0,5):\n",
    "        ax[j][i].hist(np.divide(power_lengths[k],  m_to_d), bins=20, color='black')\n",
    "        k += 1\n",
    "fig.suptitle(\"Top Ten User Dyad Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_reduced = pd.read_feather(\"recips.feather\")\n",
    "non_reduced = pd.read_feather(\"nonrecips.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10), dpi= 200, facecolor='w', edgecolor='k')\n",
    "hist, bins, _ = ax[0][0].hist(np.divide(non_reduced.length_d,  m_to_d), bins=35, color='red')\n",
    "logbins = np.logspace(np.log10(bins[1]),np.log10(bins[-1]),len(bins))\n",
    "ax[0][1].hist(np.divide(non_reduced.length_d,  m_to_d), bins=logbins, color='red')\n",
    "ax[0][0].set_ylabel(\"Quantity\")\n",
    "ax[0][0].set_xlabel(\"Days of relationship\")\n",
    "ax[0][0].set_title(\"Dyadic Distribution (Non-Reciprocal)\")\n",
    "ax[0][1].set_ylabel(\"Quantity\")\n",
    "ax[0][1].set_xlabel(\"Days of relationship\")\n",
    "ax[0][0].set_xlim(left = 0, right = 3500)\n",
    "ax[0][1].set_title(\"Logarithmic Dyadic Distribution (Non-Reciprocal)\")\n",
    "ax[0][1].set_xscale(\"log\")\n",
    "\n",
    "hist, bins, _ = ax[1][0].hist(np.divide(rec_reduced.length_d,  m_to_d), bins=25, color='blue')\n",
    "logbins = np.logspace(np.log10(bins[1]),np.log10(bins[-1]),len(bins))\n",
    "ax[1][1].hist(np.divide(rec_reduced.length_d, m_to_d), bins=logbins, color='blue')\n",
    "ax[1][0].set_ylabel(\"Quantity\")\n",
    "ax[1][0].set_xlabel(\"Days of relationship\")\n",
    "ax[1][0].set_title(\"Dyadic Distribution (Reciprocal)\")\n",
    "ax[1][0].set_xlim(left = 0, right = 3500)\n",
    "ax[1][1].set_ylabel(\"Quantity\")\n",
    "ax[1][1].set_xlabel(\"Days of relationship\")\n",
    "ax[1][1].set_title(\"Logarithmic Dyadic Distribution (Reciprocal)\")\n",
    "ax[1][1].set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup/Archiving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u_sample  = pd.read_feather(\"u2u_sample.feather\")[[\"from\", \"to\", \"at\"]]\n",
    "u2u_reduced = pd.read_feather(\"u2u_reduced.feather\")\n",
    "recips = pd.read_feather(\"recips.feather\")[[\"from\", \"to\", \"length_d\", \"num_rel\"]]\n",
    "nonrecips = pd.read_feather(\"nonrecips.feather\")[[\"from\", \"to\", \"length_d\", \"num_rel\"]]\n",
    "print(\"Intx Ct: {} \\t Dyad Ct: {} \\t Recip Prop: {}/{} - {}%\".format(len(u2u_sample), len(u2u_reduced), len(recips), len(nonrecips), 100*(len(recips)/len(nonrecips))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u2u_sample.to_hdf(\"u2u.h5\", key=\"u2u_sample\", mode=\"w\")\n",
    "u2u_reduced.to_hdf(\"dyads.h5\", key=\"u2u_reduced\", mode=\"w\")\n",
    "recips.to_hdf(\"rec_dyads.h5\", key=\"recips\", mode=\"w\")\n",
    "nonrecips.to_hdf(\"nonrec_dyads.h5\", key=\"nonrecips\", mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
