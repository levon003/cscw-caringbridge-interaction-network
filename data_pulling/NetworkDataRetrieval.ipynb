{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis Data Retrieval\n",
    "Pull all the necessary data for building a graph of the social connections of the CaringBridge network. CSV data is stored in <code>/home/shared/caringbridge/data/projects/sna-social-support/csv_data</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "import csv as csv\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guestbook_fname = \"/home/srivbane/shared/caringbridge/data/raw/guestbook_scrubbed.json\"\n",
    "journal_fname = \"/home/srivbane/shared/caringbridge/data/raw/journal.json\"\n",
    "out_path = \"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/csv_data/\"\n",
    "chunk_size = 100000\n",
    "allow = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guestbook\n",
    "The guestbook JSON has 829 chunks as of 2/14/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'829 chunks of 100000 processed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Finished!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if allow:\n",
    "    processed_count = 0; chunk_count = 0; gb = [];\n",
    "    header = [\"from_userId\", \"siteId\", \"to_userId\", \"connectionType\", \"createdAt\"]\n",
    "    with open(guestbook_fname, encoding=\"utf-8\") as infile, \\\n",
    "    open(os.path.join(out_path, \"pcts.csv\"), encoding=\"utf-8\") as supp, \\\n",
    "    open(os.path.join(out_path, \"gb.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "        csv_out = csv.writer(outfile)\n",
    "        csv_out.writerow(header)\n",
    "        pcts_r = csv.reader(supp)\n",
    "        for line in infile:\n",
    "            gb_json = json.loads(line)\n",
    "            \"\"\"\n",
    "            supp.seek(0)\n",
    "            for line in pcts_r:\n",
    "                if (line[1] == gb_json[\"siteId\"]):\n",
    "            \"\"\"\n",
    "            gb.append((int(gb_json[\"userId\"]), int(gb_json[\"siteId\"]), \"guestbook\", int(gb_json[\"createdAt\"][\"$date\"])))\n",
    "            processed_count += 1\n",
    "            if processed_count == chunk_size:\n",
    "                chunk_count += 1\n",
    "                csv_out.writerows(gb)\n",
    "                clear_output(); display(\"\" + str(chunk_count) + \" chunks of \" + str(chunk_size) + \" processed.\");\n",
    "                gb = []; processed_count = 0;\n",
    "        csv_out.writerows(gb)\n",
    "display(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the Guestbook Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"/home/srivbane/shared/caringbridge/data/projects/sna-social-support/csv_data/pcts.csv\")\n",
    "site_user_map = {}\n",
    "for siteId, group in users.groupby(by='siteId', sort=False):\n",
    "    userIds = tuple(group.userId.tolist())\n",
    "    site_user_map[siteId] = userIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed = 0\n",
    "with open(os.path.join(out_path, 'gb.csv'), 'r') as infile:\n",
    "    with open(os.path.join(out_path, 'gb1.csv'), 'w') as outfile:\n",
    "        csv_r = csv.reader(infile)\n",
    "        csv_o = csv.writer(outfile)\n",
    "        for line in csv_r:\n",
    "            try:\n",
    "                from_userId, siteId, c_type, createdAt = line\n",
    "                for to_userId in site_user_map[int(siteId)]:\n",
    "                    csv_o.writerow((from_userId, siteId, to_userId, c_type, createdAt)) # format the lines\n",
    "                    processed += 1 \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journals\n",
    "The journal JSON has 153 chunks as of 2/14/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'153 chunks of 100000 processed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "header = [\"userId\", \"siteId\", \"connectionType\", \"createdAt\"]\n",
    "with open(journal_fname, encoding=\"utf-8\") as infile:\n",
    "    processed_count = 0;\n",
    "    chunk_count = 0;\n",
    "    jo = [];\n",
    "    for line in infile:\n",
    "        jo_json = json.loads(line)\n",
    "        try:\n",
    "            jo.append((int(jo_json[\"userId\"]), int(jo_json[\"siteId\"]), \"journal\", int(jo_json[\"createdAt\"][\"$date\"])))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        finally:\n",
    "            processed_count += 1\n",
    "            if processed_count == chunk_size:\n",
    "                chunk_count += 1\n",
    "                clear_output(); display(\"\" + str(chunk_count) + \" chunks of \" + str(chunk_size) + \" processed.\");           \n",
    "                processed_count = 0;\n",
    "with open(os.path.join(out_path, \"jo.csv\"), \"w\", encoding=\"utf-8\") as outfile:\n",
    "    j = pd.DataFrame(jo, columns = header);\n",
    "    j = j.sort_values(by=[\"userId\"]);\n",
    "    j.to_csv(outfile, header=True, index=False)\n",
    "display(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal Replies\n",
    "The journal JSON has 153 chunks as of 2/14/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'152 chunks of 100000 processed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Timetraveling reply!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if allow:\n",
    "    processed_count = 0; chunk_count = 0; jr = []; offset = 0;\n",
    "    header = [\"from_userId\", \"siteId\", \"to_userID\", \"connectionType\", \"createdAt\"]\n",
    "    with open(journal_fname, encoding=\"utf-8\") as infile, open(os.path.join(out_path, \"jr.csv\"), \"w\", newline=\"\") as outfile:\n",
    "        csv_out = csv.writer(outfile)\n",
    "        csv_out.writerow(header)\n",
    "        for line in infile:\n",
    "            jo_json = json.loads(line)\n",
    "            try:\n",
    "                jr_json = jo_json[\"replies\"]\n",
    "                for reply in jr_json:\n",
    "                    reply_ts = int(1000 * datetime.strptime(reply[\"createdAt\"], \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "                    journ_ts = jo_json[\"createdAt\"][\"$date\"]\n",
    "                    if reply_ts > journ_ts and offset == 0:\n",
    "                        jr.append((int(reply[\"userId\"]), int(jo_json[\"siteId\"]), int(jo_json[\"userId\"]), \"reply\", reply_ts))\n",
    "                    else:\n",
    "                        if (offset == 0):\n",
    "                            offset = journ_ts - reply_ts\n",
    "                        jr.append((int(reply[\"userId\"]), int(jo_json[\"siteId\"]), int(jo_json[\"userId\"]), \"reply\", reply_ts + offset))\n",
    "                        display(\"Timetraveling reply!\")\n",
    "                offset = 0;\n",
    "            except KeyError:\n",
    "                pass\n",
    "            finally:\n",
    "                processed_count += 1\n",
    "                if processed_count == chunk_size:\n",
    "                    chunk_count += 1\n",
    "                    csv_out.writerows(jr)\n",
    "                    clear_output(); display(\"\" + str(chunk_count) + \" chunks of \" + str(chunk_size) + \" processed.\");\n",
    "                    jr = []; processed_count = 0;\n",
    "        csv_out.writerows(jr)\n",
    "display(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amps\n",
    "The journal JSON has 153 chunks as of 2/14/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'153 chunks of 100000 processed.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Finished!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if allow:\n",
    "    processed_count = 0; chunk_count = 0; amps = [];\n",
    "    header = [\"from_userId\", \"siteId\", \"to_userId\", \"connectionType\", \"createdAt\"]\n",
    "    with open(journal_fname, encoding=\"utf-8\") as infile, open(os.path.join(out_path, \"amps.csv\"), \"w\", newline=\"\") as outfile:\n",
    "        csv_out = csv.writer(outfile)\n",
    "        csv_out.writerow(header)\n",
    "        for line in infile:\n",
    "            jo_json = json.loads(line)\n",
    "            try:\n",
    "                if len(jo_json[\"amps\"]) > 0:\n",
    "                    for amp in jo_json[\"amps\"]:\n",
    "                        amps.append((amp, int(jo_json[\"siteId\"]), int(jo_json[\"userId\"]), \"amps\", int(jo_json[\"createdAt\"][\"$date\"])))\n",
    "            except KeyError:\n",
    "                pass\n",
    "            finally:\n",
    "                processed_count += 1\n",
    "                if processed_count == chunk_size:\n",
    "                    chunk_count += 1\n",
    "                    csv_out.writerows(amps)\n",
    "                    clear_output(); display(\"\" + str(chunk_count) + \" chunks of \" + str(chunk_size) + \" processed.\");           \n",
    "                    amps = []; processed_count = 0;\n",
    "        csv_out.writerows(amps)\n",
    "display(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mapping User ID to Site ID\n",
    "The journal JSON has 153 chunks as of 2/14/2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'153 / 153'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[ OK ] Decoding phase complete. Entering sorting phase.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if allow:\n",
    "    processed_count = 0; chunk_count = 0; lib = {}; pcts = []; count = 0\n",
    "    with open(journal_fname, encoding=\"utf-8\") as infile, \\\n",
    "    open(os.path.join(out_path, \"a.csv\"), \"w\", newline=\"\") as outfile:\n",
    "        csv_a = csv.writer(outfile)\n",
    "        for line in infile:\n",
    "            jo_json = json.loads(line)\n",
    "            csv_a.writerow((jo_json[\"userId\"], jo_json[\"siteId\"]))\n",
    "            processed_count += 1\n",
    "            if processed_count == chunk_size:\n",
    "                chunk_count += 1\n",
    "                processed_count = 0\n",
    "                clear_output(); display(str(chunk_count) + \" / 153\");\n",
    "display(\"[ OK ] Decoding phase complete. Entering sorting phase.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'153 / 153'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[ OK ] Reduction phase complete. Let me tabulate the percentage for each tuple.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if allow:\n",
    "    with open(os.path.join(out_path, \"a.csv\"), \"r\", newline=\"\") as infile, \\\n",
    "    open(os.path.join(out_path, \"srt.csv\"), \"w\", newline=\"\") as outfile:\n",
    "        cmd = [\"sort\", \"-k1\", \"-n\", \"-t,\", os.path.join(out_path, \"a.csv\")]\n",
    "        subprocess.call(cmd, stdout=outfile)\n",
    "        clear_output(); display(\"[ OK ] Sorting phase complete. Entering reduction phase.\");\n",
    "    with open(os.path.join(out_path, \"srt.csv\"), \"r\", newline=\"\") as infile, \\\n",
    "    open(os.path.join(out_path, \"b.csv\"), \"w\", newline=\"\") as outfile:\n",
    "        interim = csv.reader(infile)\n",
    "        sort = csv.writer(outfile)\n",
    "        uid = 0; sid = 0; tot = 0; processed_count = 0; chunk_count = 0;\n",
    "        for row in interim:\n",
    "            if (row[0] != uid or row[1] != sid):\n",
    "                sort.writerow((uid, sid, tot))\n",
    "                uid = row[0]; sid = row[1]\n",
    "                tot = 0\n",
    "            tot += 1\n",
    "            processed_count += 1\n",
    "            if processed_count == chunk_size:\n",
    "                chunk_count += 1\n",
    "                processed_count = 0\n",
    "                clear_output(); display(str(chunk_count) + \" / 153\" );\n",
    "display(\"[ OK ] Reduction phase complete. Let me tabulate the percentage for each tuple.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6 chunks processed from reduced CSV.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'[ OK ] Finished! 3.1028281619530635% of tuples with non-exclusive authors.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = 0\n",
    "if allow:\n",
    "    with open(os.path.join(out_path, \"b.csv\"), \"r\", newline=\"\") as infile, \\\n",
    "    open(os.path.join(out_path, \"pcts.csv\"), \"w\", newline=\"\") as outfile:\n",
    "        interim = csv.reader(infile)\n",
    "        fin = csv.writer(outfile)\n",
    "        tot = 0; processed_count = 0; chunk_count = 0; rcd = {\"uid\" : 0, \"sites\" : [] ,\"quants\" : []}\n",
    "        fin.writerow((\"userId\", \"siteId\", \"numJournals\", \"pctJournals\"))\n",
    "        for row in interim:\n",
    "            if (row[0] != rcd[\"uid\"]):\n",
    "                pct_flag = True\n",
    "                for i in range(len(rcd[\"sites\"])):\n",
    "                    pct = int(rcd[\"quants\"][i]) / tot\n",
    "                    if (pct_flag and pct != 1.0):\n",
    "                        c += 1\n",
    "                        pct_flag = False\n",
    "                    fin.writerow((rcd[\"uid\"], rcd[\"sites\"][i], int(rcd[\"quants\"][i]), pct))\n",
    "                rcd = {\"uid\" : row[0], \"sites\" : [], \"quants\" : []}\n",
    "                tot = 0\n",
    "            rcd[\"sites\"].append(row[1]); rcd[\"quants\"].append(row[2]); tot += int(row[2])\n",
    "            processed_count += 1\n",
    "            if processed_count == chunk_size:\n",
    "                chunk_count += 1\n",
    "                processed_count = 0\n",
    "                clear_output(); display(str(chunk_count) + \" chunks processed from reduced CSV.\" );\n",
    "        pct_flag = True\n",
    "        for i in range(len(rcd[\"sites\"])):\n",
    "            pct = int(rcd[\"quants\"][i]) / tot\n",
    "            if (pct_flag and pct != 1.0):\n",
    "                c += 1\n",
    "                pct_flag = False\n",
    "            fin.writerow((rcd[\"uid\"], rcd[\"sites\"][i], int(rcd[\"quants\"][i]), pct))\n",
    "    os.remove(os.path.join(out_path, \"a.csv\")); os.remove(os.path.join(out_path, \"b.csv\")); os.remove(os.path.join(out_path, \"srt.csv\"));\n",
    "display(\"[ OK ] Finished! \" + str(100 * c / (chunk_size * chunk_count + len(rcd[\"sites\"]))) + \"% of tuples with non-exclusive authors.\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-conda)",
   "language": "python",
   "name": "shared-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
